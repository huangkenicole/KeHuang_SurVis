define({ entries : {
    "BramerWM2018SEaM": {
        "abstract": "Background: Researchers performing systematic reviews (SRs) must carefully consider the relevance of thousands of citations retrieved from bibliographic database searches, the majority of which will be excluded later on close inspection. Well-developed bibliographic searches are generally created with thesaurus or index terms in combination with keywords found in the title and/or abstract fields of citation records. Records in the bibliographic database Embase contain many more thesaurus terms than MEDLINE. Here, we aim to examine how limiting searches to major thesaurus terms (in MEDLINE called focus terms) in Embase and MEDLINE as well as limiting to words in the title and abstract fields of those databases affects the overall recall of SR searches. Methods: To examine the impact of using search techniques aimed at higher precision, we analyzed previously completed SRs and focused our original searches to major thesaurus terms or terms in title and/or abstract only in Embase.com or in Embase.com and MEDLINE (Ovid) combined. We examined the total number of search results in both Embase and MEDLINE and checked whether included references were retrieved by these more focused approaches. Results: For 73 SRs, we limited Embase searches to major terms only while keeping the search in MEDLINE and other databases such as Web of Science as they were. The overall search yield (or total number of search results) was reduced by 8%. Six reviews (9%) lost more than 5% of the relevant references. Limiting Embase and MEDLINE to major thesaurus terms, the number of references was 13% lower. For 15% of the reviews, the loss of relevant references was more than 5%. Searching Embase for title and abstract caused a loss of more than 5% in 16 reviews (22%), while limiting Embase and MEDLINE that way this happened in 24 reviews (33%). Conclusions: Of the four search options, two options substantially reduced the overall search yield. However, this also resulted in a greater chance of losing relevant references, even though many references were still found in other databases such as Web of Science.",
        "address": "LONDON",
        "author": "Bramer, WM and Giustini, D and Kleijnen, J and Franco Duran, OH",
        "copyright": "info:eu-repo/semantics/openAccess",
        "issn": "2046-4053",
        "journal": "Systematic reviews",
        "keywords": "Bibliographic ; Bibliographic data bases ; Databases Bibliographic ; General & Internal Medicine ; Humans ; Information storage ; Information storage and retrieval ; Information Storage and Retrieval - methods ; Life Sciences & Biomedicine ; Medical Subject Headings-MeSH ; Medicine General & Internal ; MEDLINE ; Methodology ; Preventive medicine ; Prospective Studies ; Researchers ; Review literature as topic ; Science & Technology ; Search Engine ; Sensitivity and specificity ; Subject heading schemes ; Subject Headings ; Systematic review ; Systematic Reviews as Topic ; Vocabularies & taxonomies",
        "language": "eng",
        "number": "1",
        "pages": "200--200",
        "publisher": "Springer Nature",
        "title": "Searching Embase and MEDLINE by using only major descriptors or title and abstract fields: a prospective exploratory study",
        "type": "article",
        "volume": "7",
        "year": "2018"
    },
    "CohenA.M.2006RWiS": {
        "abstract": "To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease. A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class\u2013specific evidence or not. Cross-validation experiments were performed to evaluate performance. Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95% recall was used as the measure of value to the review process. A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50% or greater. Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.",
        "address": "OXFORD",
        "author": "Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin",
        "copyright": "2006 American Medical Informatics Association",
        "issn": "1067-5027",
        "journal": "Journal of the American Medical Informatics Association : JAMIA",
        "keywords": "Artificial Intelligence ; Bibliometrics ; Computer Science ; Computer Science Information Systems ; Computer Science Interdisciplinary Applications ; Evidence-Based Medicine ; Health Care Sciences & Services ; Information Science & Library Science ; Information Storage and Retrieval - methods ; Life Sciences & Biomedicine ; Medical colleges ; Medical Informatics ; Meta-Analysis as Topic ; Original Investigations ; Pharmaceutical Preparations ; Publications - classification ; Review Literature as Topic ; Science & Technology ; Technology ; Workload",
        "language": "eng",
        "number": "2",
        "pages": "206--219",
        "publisher": "Elsevier Inc",
        "title": "Reducing Workload in Systematic Review Preparation Using Automated Citation Classification",
        "type": "article",
        "volume": "13",
        "year": "2006"
    },
    "GoossenKaethe2020Dctr": {
        "abstract": "Background When conducting an Overviews of Reviews on health-related topics, it is unclear which combination of bibliographic databases authors should use for searching for SRs. Our goal was to determine which databases included the most systematic reviews and identify an optimal database combination for searching systematic reviews. Methods A set of 86 Overviews of Reviews with 1219 included systematic reviews was extracted from a previous study. Inclusion of the systematic reviews was assessed in MEDLINE, CINAHL, Embase, Epistemonikos, PsycINFO, and TRIP. The mean inclusion rate (% of included systematic reviews) and corresponding 95% confidence interval were calculated for each database individually, as well as for combinations of MEDLINE with each other database and reference checking. Results Inclusion of systematic reviews was higher in MEDLINE than in any other single database (mean inclusion rate 89.7%; 95% confidence interval [89.0-90.3%]). Combined with reference checking, this value increased to 93.7% [93.2-94.2%]. The best combination of two databases plus reference checking consisted of MEDLINE and Epistemonikos (99.2% [99.0-99.3%]). Stratification by Health Technology Assessment reports (97.7% [96.5-98.9%]) vs. Cochrane Overviews (100.0%) vs. non-Cochrane Overviews (99.3% [99.1-99.4%]) showed that inclusion was only slightly lower for Health Technology Assessment reports. However, MEDLINE, Epistemonikos, and reference checking remained the best combination. Among the 10/1219 systematic reviews not identified by this combination, five were published as websites rather than journals, two were included in CINAHL and Embase, and one was included in the database ERIC. Conclusions MEDLINE and Epistemonikos, complemented by reference checking of included studies, is the best database combination to identify systematic reviews on health-related topics.",
        "address": "LONDON",
        "author": "Goossen, Kaethe and Hess, Simone and Lunny, Carole and Pieper, Dawid",
        "copyright": "2020. This work is licensed under http://creativecommons.org/licenses/by/4.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "1471-2288",
        "journal": "BMC medical research methodology",
        "keywords": "Algorithms ; Clinical trials ; Collaboration ; Cost analysis ; Data collection ; Health Care Sciences & Services ; Internet resources ; Intervention ; Library collections ; Life Sciences & Biomedicine ; Overview of reviews ; Review methods ; Science & Technology ; Search strategy ; Social sciences ; Systematic reviews ; Umbrella review",
        "language": "eng",
        "number": "1",
        "pages": "1--138",
        "publisher": "Springer Nature",
        "title": "Database combinations to retrieve systematic reviews in overviews of reviews: a methodological study",
        "type": "article",
        "volume": "20",
        "year": "2020"
    },
    "HarrisonHannah2020Stts": {
        "abstract": "Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T&Ab screening for systematic reviews within healthcare research. We identified software tools using three search methods: a web-based search; a search of the online \"systematic review toolbox\"; and screening of references in existing literature. We included tools that were accessible and available for testing at the time of the study (December 2018), do not require specific computing infrastructure and provide basic screening functionality for systematic reviews. Key properties of each software tool were identified using a feature analysis adapted for this purpose. This analysis included a weighting developed by a group of medical researchers, therefore prioritising the most relevant features. The highest scoring tools from the feature analysis were then included in a user survey, in which we further investigated the suitability of the tools for supporting T&Ab screening amongst systematic reviewers working in medical research.",
        "address": "England",
        "author": "Harrison, Hannah and Griffin, Simon J and Kuhn, Isla and Usher-Smith, Juliet A",
        "copyright": "2020. This work is licensed under http://creativecommons.org/licenses/by/4.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "fifteen tools met our inclusion criteria. they vary significantly in relation to cost, scope and intended user community. six of the identified tools (abstrackr, colandr, covidence, dragon, eppi-reviewer and rayyan) scored higher than 75% in the feature analysis and were included in the user survey. of these, covidence and rayyan were the most popular with the survey respondents. their usability scored highly across a range of metrics, with all surveyed researchers (n": "6) stating that they would be likely (or very likely) to use these tools in the future. Based on this study, we would recommend Covidence and Rayyan to systematic reviewers looking for suitable and easy to use tools to support T&Ab screening within healthcare research. These two tools consistently demonstrated good alignment with user requirements. We acknowledge, however, the role of some of the other tools we considered in providing more specialist features that may be of great importance to many researchers.",
        "issn": "1471-2288",
        "journal": "BMC medical research methodology",
        "keywords": "Abstracting and Indexing - methods ; Biomedical Research ; Collaboration ; Delivery of Health Care ; Discussion groups ; Evidence-Based Medicine - methods ; Feature analysis ; Humans ; Medical research ; Researchers ; Screening ; Search strategies ; Software ; Software engineering ; Software tools ; Surveys and Questionnaires ; Systematic review ; Systematic reviews ; Systematic Reviews as Topic - methods ; Title and abstract ; User surveys",
        "language": "eng",
        "number": "1",
        "pages": "7--7",
        "publisher": "BioMed Central",
        "title": "Software tools to support title and abstract screening for systematic reviews in healthcare: an evaluation",
        "type": "article",
        "volume": "20",
        "year": "2020"
    },
    "KiritchenkoSvetlana2010EAeo": {
        "abstract": "Background: Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). Methods: ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. Results: We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88% of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80% of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93% and 91%, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94%), with a majority of these (696) representing fully correct and complete answers. Conclusions: Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols).",
        "address": "LONDON",
        "author": "Kiritchenko, Svetlana and De Bruijn, Berry and Carini, Simona and Martin, Joel and Sim, Ida",
        "copyright": "Copyright 2020 Elsevier B.V., All rights reserved.",
        "issn": "1472-6947",
        "journal": "BMC medical informatics and decision making",
        "keywords": "Clinical trials ; Data entry ; Data mining ; Design ; Humans ; Information Storage and Retrieval - methods ; Information Storage and Retrieval - standards ; Internal medicine ; Life Sciences & Biomedicine ; Medical Informatics ; Medical publishing ; Medicine ; Periodicals as Topic ; Programming languages ; Randomized Controlled Trials as Topic ; Reproducibility of Results ; Science & Technology ; Semantics ; Studies ; Technical Advance ; User interface",
        "language": "eng",
        "number": "1",
        "pages": "56--56",
        "publisher": "Springer Nature",
        "title": "ExaCT: Automatic extraction of clinical trial characteristics from journal publications",
        "type": "article",
        "volume": "10",
        "year": "2010"
    },
    "MarshallIainJ.2015ARoB": {
        "abstract": "Systematic reviews, which summarize the entirety of the evidence pertaining to a specific clinical question, have become critical for evidence-based decision making in healthcare. But such reviews have become increasingly onerous to produce due to the exponentially expanding biomedical literature base. This study proposes a step toward mitigating this problem by automating risk of bias assessment in systematic reviews, in which reviewers determine whether study results may be affected by biases (e.g., poor randomization or blinding). Conducting risk of bias assessment is an important but onerous task. We thus describe a machine learning approach to automate this assessment, using the standard Cochrane Risk of Bias Tool which assesses seven common types of bias. Training such a system would typically require a large labeled corpus, which would be prohibitively expensive to collect here. Instead, we use distant supervision, using data from the Cochrane Database of Systematic Reviews (a large repository of systematic reviews), to pseudoannotate a corpus of 2200 clinical trial reports in PDF format. We then develop a joint model which, using the full text of a clinical trial report as input, predicts the risks of bias while simultaneously extracting the text fragments supporting these assessments. This study represents a step toward automating or semiautomating extraction of data necessary for the synthesis of clinical trials.",
        "address": "PISCATAWAY",
        "author": "Marshall, Iain J. and Kuiper, Joel and Wallace, Byron C.",
        "copyright": "Copyright 2015 Elsevier B.V., All rights reserved.",
        "issn": "2168-2194",
        "journal": "IEEE journal of biomedical and health informatics",
        "keywords": "Assessments ; Automation ; Bias ; Clinical trials ; Clinical Trials as Topic ; Computer Science ; Computer Science Information Systems ; Computer Science Interdisciplinary Applications ; Evidence-based medicine ; Feature extraction ; health informatics ; Humans ; Informatics ; Joints ; Life Sciences & Biomedicine ; Literature reviews ; machine learning ; Mathematical & Computational Biology ; Medical Informatics ; Medical Informatics - methods ; Medical research ; Natural Language Processing ; Random sequences ; Resource management ; Risk ; Risk assessment ; Risk Assessment - methods ; Science & Technology ; Systematics ; Technology ; Texts",
        "language": "eng",
        "number": "4",
        "pages": "1406--1412",
        "publisher": "IEEE",
        "title": "Automating Risk of Bias Assessment for Clinical Trials",
        "type": "article",
        "volume": "19",
        "year": "2015"
    },
    "MarshallIainJ2016Reoa": {
        "abstract": "Objective To develop and evaluate RobotReviewer, a machine learning (ML) system that automatically assesses bias in clinical trials. From a (PDF-formatted) trial report, the system should determine risks of bias for the domains defined by the Cochrane Risk of Bias (RoB) tool, and extract supporting text for these judgments. Methods We algorithmically annotated 12,808 trial PDFs using data from the Cochrane Database of Systematic Reviews (CDSR). Trials were labeled as being at low or high/unclear risk of bias for each domain, and sentences were labeled as being informative or not. This dataset was used to train a multi-task ML model. We estimated the accuracy of ML judgments versus humans by comparing trials with two or more independent RoB assessments in the CDSR. Twenty blinded experienced reviewers rated the relevance of supporting text, comparing ML output with equivalent (human-extracted) text from the CDSR. Results By retrieving the top 3 candidate sentences per document (top3 recall), the best ML text was rated more relevant than text from the CDSR, but not significantly (60.4% ML text rated 'highly relevant' v 56.5% of text from reviews; difference +3.9%, [-3.2% to +10.9%]). Model RoB judgments were less accurate than those from published reviews, though the difference was Conclusion Risk of bias assessment may be automated with reasonable accuracy. Automatically identified text supporting bias assessment is of equal quality to the manually identified text in the CDSR. This technology could substantially reduce reviewer workload and expedite evidence syntheses.",
        "address": "OXFORD",
        "author": "Marshall, Iain J and Kuiper, Joel and Wallace, Byron C",
        "copyright": "info:eu-repo/semantics/openAccess",
        "issn": "1067-5027",
        "journal": "Journal of the American Medical Informatics Association : JAMIA",
        "keywords": "Algorithms ; Bias ; Clinical Trials as Topic ; Computer Science ; Computer Science Information Systems ; Computer Science Interdisciplinary Applications ; Data Mining ; Databases as Topic ; Health Care Sciences & Services ; Information Science & Library Science ; Life Sciences & Biomedicine ; Machine Learning ; Medical Informatics ; Natural Language Processing ; Peer Review Research - methods ; Randomized controlled trials as topic ; Research and Applications ; Review Literature as Topic ; Science & Technology ; Systematic review ; Technology",
        "language": "eng",
        "number": "1",
        "pages": "193--201",
        "publisher": "Oxford University Press",
        "title": "RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials",
        "type": "article",
        "volume": "23",
        "year": "2016"
    },
    "NovoaJorge2023PIRo": {
        "abstract": "Scientific knowledge is being accumulated in the biomedical literature at an unprecedented pace. The most widely used database with biomedicine-related article abstracts, PubMed, currently contains more than 36 million entries. Users performing searches in this database for a subject of interest face thousands of entries (articles) that are difficult to process manually. In this work, we present an interactive tool for automatically digesting large sets of PubMed articles: PMIDigest (PubMed IDs digester). The system allows for classification/sorting of articles according to different criteria, including the type of article and different citation-related figures. It also calculates the distribution of MeSH (medical subject headings) terms for categories of interest, providing in a picture of the themes addressed in the set. These MeSH terms are highlighted in the article abstracts in different colors depending on the category. An interactive representation of the interarticle citation network is also presented in order to easily locate article \"clusters\" related to particular subjects, as well as their corresponding \"hub\" articles. In addition to PubMed articles, the system can also process a set of Scopus or Web of Science entries. In summary, with this system, the user can have a \"bird's eye view\" of a large set of articles and their main thematic tendencies and obtain additional information not evident in a plain list of abstracts.",
        "address": "Switzerland",
        "author": "Novoa, Jorge and Chagoyen, M\u00f3nica and Benito, Carlos and Moreno, F. Javier and Pazos, Florencio",
        "copyright": "Copyright 2023 Elsevier B.V., All rights reserved.",
        "issn": "2073-4425",
        "journal": "Genes",
        "keywords": "Bibliometrics ; Breast cancer ; citation databases ; Citations ; data mining ; Databases Factual ; Datasets ; Documentation ; Humans ; Keywords ; literature digest ; Microorganisms ; PubMed ; scientific literature ; Visualization",
        "language": "eng",
        "number": "4",
        "pages": "942",
        "publisher": "MDPI AG",
        "title": "PMIDigest: Interactive Review of Large Collections of PubMed Entries to Distill Relevant Information",
        "type": "article",
        "volume": "14",
        "year": "2023"
    },
    "WallaceByron2012Daim": {
        "abstract": "Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.",
        "author": "Wallace, Byron and Small, Kevin and Brodley, Carla and Lau, Joseph and Trikalinos, Thomas",
        "booktitle": "IHI'12 - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium",
        "copyright": "Copyright 2012 Elsevier B.V., All rights reserved.",
        "isbn": "9781450307819",
        "keywords": "Active learning ; Applications ; Evidence-based medicine ; Machine learning ; Medical ; Text classiffcation ; text classification",
        "language": "eng",
        "pages": "819--824",
        "publisher": "ACM",
        "series": "IHI '12",
        "title": "Deploying an interactive machine learning system in an evidence-based practice center: abstrackr",
        "type": "inproceedings",
        "year": "2012"
    },
    "WallaceByronC.2010Ssob": {
        "abstract": "Background: Systematic reviews address a specific clinical question by unbiasedly assessing and analyzing the pertinent literature. Citation screening is a time-consuming and critical step in systematic reviews. Typically, reviewers must evaluate thousands of citations to identify articles eligible for a given review. We explore the application of machine learning techniques to semi-automate citation screening, thereby reducing the reviewers' workload. Results: We present a novel online classification strategy for citation screening to automatically discriminate \"relevant\" from \"irrelevant\" citations. We use an ensemble of Support Vector Machines (SVMs) built over different feature-spaces (e.g., abstract and title text), and trained interactively by the reviewer(s). Semi-automating the citation screening process is difficult because any such strategy must identify all citations eligible for the systematic review. This requirement is made harder still due to class imbalance; there are far fewer \"relevant\" than \"irrelevant\" citations for any given systematic review. To address these challenges we employ a custom active-learning strategy developed specifically for imbalanced datasets. Further, we introduce a novel under-sampling technique. We provide experimental results over three real-world systematic review datasets, and demonstrate that our algorithm is able to reduce the number of citations that must be screened manually by nearly half in two of these, and by around 40% in the third, without excluding any of the citations eligible for the systematic review. Conclusions: We have developed a semi-automated citation screening algorithm for systematic reviews that has the potential to substantially reduce the number of citations reviewers have to manually screen, without compromising the quality and comprehensiveness of the review.",
        "address": "LONDON",
        "author": "Wallace, Byron C. and Trikalinos, Thomas A. and Lau, Joseph and Brodley, Carla and Schmid, Christopher H.",
        "copyright": "Copyright 2013 Elsevier B.V., All rights reserved.",
        "issn": "1471-2105",
        "journal": "BMC bioinformatics",
        "keywords": "Algorithms ; Applications software ; Automation ; Biochemical Research Methods ; Biochemistry & Molecular Biology ; Biotechnology & Applied Microbiology ; Computer science ; Information Storage and Retrieval - methods ; Life Sciences & Biomedicine ; Mathematical & Computational Biology ; Medical research ; Periodicals as Topic ; Research article ; Review Literature as Topic ; Science & Technology ; Studies ; Teaching methods",
        "language": "eng",
        "number": "1",
        "pages": "55--55",
        "publisher": "Springer Nature",
        "title": "Semi-automated screening of biomedical citations for systematic reviews",
        "type": "article",
        "volume": "11",
        "year": "2010"
    }
}});